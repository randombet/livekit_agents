{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# OpenAI Realtime Voice Agent with Tools (LiveKit Agents)\n\nThis notebook demonstrates a real-time voice assistant using **LiveKit Agents** framework with:\n- **OpenAI Realtime API**: GPT-4o realtime model with server-side turn detection\n- **Echo Cancellation**: Uses LiveKit's AudioProcessingModule to prevent feedback\n- **Custom function tools**: Calculator, current time\n- **Local audio I/O**: Microphone input and speaker output via sounddevice\n\n## Requirements\n\n```bash\npip install sounddevice livekit livekit-agents livekit-plugins-openai python-dotenv\n```\n\n**macOS:** You may need PortAudio:\n```bash\nbrew install portaudio\n```\n\n## Usage\n1. Set `OPENAI_API_KEY` environment variable\n2. Run all cells in order\n3. Speak into your microphone\n4. Try: \"What's 25 times 17?\" or \"What time is it in Tokyo?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies if needed\n# !pip install sounddevice livekit livekit-agents livekit-plugins-openai python-dotenv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport asyncio\nimport math\nimport threading\nimport logging\nfrom datetime import datetime\nfrom dotenv import load_dotenv\n\nimport sounddevice as sd\nimport numpy as np\nfrom livekit import rtc\n\n# Load environment variables\nload_dotenv()\n\n# Set up logging (INFO level - set to DEBUG for verbose output)\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"voice_agent\")\n\n# Audio configuration (matching LiveKit agents console mode)\nSAMPLE_RATE = 24000\nCHANNELS = 1\nFRAME_SAMPLES = 240  # 10ms frames\nBLOCK_SIZE = 2400    # 100ms blocks\n\nprint(f\"Sample rate: {SAMPLE_RATE} Hz\")\nprint(f\"Block size: {BLOCK_SIZE} samples ({BLOCK_SIZE/SAMPLE_RATE*1000:.0f}ms)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify API key\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY not set.\\n\"\n",
    "        \"Set it with: export OPENAI_API_KEY='your-key'\"\n",
    "    )\n",
    "print(\"API key found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available audio devices\n",
    "print(\"Available audio devices:\")\n",
    "print(sd.query_devices())\n",
    "print(f\"\\nDefault input: {sd.default.device[0]}\")\n",
    "print(f\"Default output: {sd.default.device[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Import LiveKit Agents Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "from livekit.agents import Agent, AgentSession, utils\nfrom livekit.agents.voice import io\nfrom livekit.agents.voice.io import AudioOutputCapabilities\nfrom livekit.agents.voice.events import RunContext  # For tool context\nfrom livekit.agents.llm import function_tool\nfrom livekit.plugins import openai\nfrom livekit import rtc\n\nprint(\"LiveKit Agents components imported!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Define Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Custom Tools\n# =============================================================================\n# \n# NOTE: The LiveKit Agents framework uses a state machine that pauses speech\n# scheduling after the agent finishes speaking. This means background tasks\n# CANNOT inject speech after the main turn completes.\n#\n# The supported patterns are:\n# 1. Fast tools: Return immediately (like calculate, get_current_time)\n# 2. Slow tools: Block until completion (framework handles the flow)\n#\n# For slow operations, the agent will naturally speak acknowledgment\n# (\"Let me search for that...\") while the tool runs.\n# =============================================================================\n\n@function_tool\nasync def calculate(expression: str) -> str:\n    \"\"\"Evaluate a mathematical expression.\n    \n    Args:\n        expression: Math expression (e.g., '2 + 2', 'sqrt(16)', 'sin(pi/2)')\n    \"\"\"\n    allowed = {\n        'sqrt': math.sqrt, 'sin': math.sin, 'cos': math.cos,\n        'tan': math.tan, 'log': math.log, 'log10': math.log10,\n        'exp': math.exp, 'pi': math.pi, 'e': math.e,\n        'abs': abs, 'round': round, 'pow': pow,\n    }\n    try:\n        result = eval(expression, {'__builtins__': {}}, allowed)\n        return f\"The result of {expression} is {result}\"\n    except Exception as e:\n        return f\"Error: {e}\"\n\n\n@function_tool\nasync def get_current_time(timezone: str = \"local\") -> str:\n    \"\"\"Get the current date and time.\n    \n    Args:\n        timezone: Timezone name (e.g., 'UTC', 'US/Pacific'). Defaults to local.\n    \"\"\"\n    try:\n        if timezone and timezone != \"local\":\n            import pytz\n            tz = pytz.timezone(timezone)\n            now = datetime.now(tz)\n            return f\"The time in {timezone} is {now.strftime('%Y-%m-%d %H:%M:%S %Z')}\"\n        else:\n            now = datetime.now()\n            return f\"The local time is {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n    except Exception:\n        now = datetime.utcnow()\n        return f\"The UTC time is {now.strftime('%Y-%m-%d %H:%M:%S')} UTC\"\n\n\n@function_tool\nasync def slow_web_search(ctx: RunContext, query: str) -> str | None:\n    \"\"\"Search the web for information (demonstrates slow tool handling).\n    \n    This tool simulates a slow web search that takes 3 seconds.\n    The framework handles the flow: agent can speak while this runs,\n    user can interrupt, and result is spoken when ready.\n    \n    Args:\n        ctx: RunContext for speech handle access\n        query: The search query\n    \"\"\"\n    print(f\"[Tool] slow_web_search starting for: {query}\")\n    \n    # Create the slow task\n    async def _do_search():\n        await asyncio.sleep(3)  # Simulate API delay\n        return f\"Top results for '{query}': 1) AI advances in 2024, 2) New language models released, 3) Major tech announcements\"\n    \n    # Start the task\n    search_task = asyncio.ensure_future(_do_search())\n    \n    # Wait for either: task completion OR user interruption\n    # This lets the agent speak naturally while we wait\n    await ctx.speech_handle.wait_if_not_interrupted([search_task])\n    \n    if ctx.speech_handle.interrupted:\n        print(f\"[Tool] slow_web_search interrupted for: {query}\")\n        search_task.cancel()\n        return None  # Return None to skip tool reply\n    \n    result = search_task.result()\n    print(f\"[Tool] slow_web_search completed for: {query}\")\n    return result\n\n\nprint(\"Tools defined:\")\nprint(\"  - calculate: Fast math evaluation\")\nprint(\"  - get_current_time: Fast time lookup\")\nprint(\"  - slow_web_search: Slow search (3s) with interruption support\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Audio I/O Classes\n",
    "\n",
    "These classes connect sounddevice to the LiveKit Agents framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "class NotebookAudioInput(io.AudioInput):\n    \"\"\"Audio input from microphone via sounddevice.\"\"\"\n    \n    def __init__(self, loop: asyncio.AbstractEventLoop):\n        super().__init__(label=\"Notebook Microphone\")\n        self._loop = loop\n        self._audio_ch: utils.aio.Chan[rtc.AudioFrame] = utils.aio.Chan()\n        self._attached = True\n    \n    def push_frame(self, frame: rtc.AudioFrame) -> None:\n        \"\"\"Push audio frame from sounddevice callback.\"\"\"\n        if self._attached:\n            try:\n                self._audio_ch.send_nowait(frame)\n            except Exception:\n                pass\n    \n    async def __anext__(self) -> rtc.AudioFrame:\n        return await self._audio_ch.__anext__()\n    \n    def close(self):\n        self._attached = False\n        self._audio_ch.close()\n\n\nclass NotebookAudioOutput(io.AudioOutput):\n    \"\"\"Audio output to speaker via sounddevice.\n    \n    Supports pause/resume for false interruption handling.\n    Properly tracks playback state with on_playback_started/on_playback_finished.\n    \"\"\"\n    \n    def __init__(self, loop: asyncio.AbstractEventLoop):\n        super().__init__(\n            label=\"Notebook Speaker\",\n            capabilities=io.AudioOutputCapabilities(pause=True),  # Enable pause support\n            next_in_chain=None,\n            sample_rate=SAMPLE_RATE,\n        )\n        self._loop = loop\n        self._buffer = bytearray()\n        self._lock = threading.Lock()\n        self._closed = False\n        \n        # Playback tracking - CRITICAL for proper session coordination\n        self._pushed_duration: float = 0.0\n        self._capture_start: float = 0.0\n        self._flush_task: asyncio.Task | None = None\n        self._output_empty_ev = asyncio.Event()\n        self._output_empty_ev.set()\n        self._interrupted_ev = asyncio.Event()\n        \n        # Pause tracking for false interruption handling\n        self._paused_at: float | None = None\n        self._paused_duration: float = 0.0\n    \n    @property\n    def paused(self) -> bool:\n        \"\"\"Check if audio output is paused.\"\"\"\n        return self._paused_at is not None\n    \n    @property\n    def audio_lock(self) -> threading.Lock:\n        return self._lock\n    \n    @property\n    def audio_buffer(self) -> bytearray:\n        return self._buffer\n    \n    def mark_output_empty(self) -> None:\n        \"\"\"Signal that output buffer is empty.\"\"\"\n        self._output_empty_ev.set()\n    \n    async def capture_frame(self, frame: rtc.AudioFrame) -> None:\n        \"\"\"Capture audio frame from agent for playback.\"\"\"\n        await super().capture_frame(frame)\n        if self._closed:\n            return\n        \n        # Wait for any pending flush to complete\n        if self._flush_task and not self._flush_task.done():\n            logger.warning(\"capture_frame called while flush in progress\")\n            await self._flush_task\n        \n        # Signal playback started on first frame\n        if not self._pushed_duration:\n            self._capture_start = time.monotonic()\n            self.on_playback_started(created_at=time.time())\n            logger.debug(\"Playback started\")\n        \n        # Track total pushed duration and add to buffer\n        self._pushed_duration += frame.duration\n        with self._lock:\n            self._buffer.extend(frame.data)\n            self._output_empty_ev.clear()\n    \n    def flush(self) -> None:\n        \"\"\"Flush buffered audio, marking segment complete.\"\"\"\n        super().flush()\n        if self._pushed_duration:\n            if self._flush_task and not self._flush_task.done():\n                logger.warning(\"flush called while previous flush in progress\")\n                self._flush_task.cancel()\n            \n            # Wait for playout to complete\n            self._flush_task = asyncio.create_task(self._wait_for_playout())\n    \n    async def _wait_for_playout(self) -> None:\n        \"\"\"Wait for audio to finish playing, then signal playback_finished.\"\"\"\n        async def _wait_buffered_audio() -> None:\n            while len(self._buffer) > 0:\n                await self._output_empty_ev.wait()\n                await asyncio.sleep(0)\n        \n        wait_for_interruption = asyncio.create_task(self._interrupted_ev.wait())\n        wait_for_playout = asyncio.create_task(_wait_buffered_audio())\n        \n        try:\n            await asyncio.wait(\n                [wait_for_playout, wait_for_interruption],\n                return_when=asyncio.FIRST_COMPLETED,\n            )\n            interrupted = wait_for_interruption.done()\n        finally:\n            wait_for_playout.cancel()\n            wait_for_interruption.cancel()\n        \n        # Account for any paused time\n        if self._paused_at is not None:\n            self._paused_duration += time.monotonic() - self._paused_at\n            self._paused_at = None\n        \n        # Calculate actual played duration\n        if interrupted:\n            played_duration = time.monotonic() - self._capture_start - self._paused_duration\n            played_duration = min(max(0, played_duration), self._pushed_duration)\n            logger.debug(f\"Playback interrupted after {played_duration:.2f}s\")\n        else:\n            played_duration = self._pushed_duration\n            logger.debug(f\"Playback completed: {played_duration:.2f}s\")\n        \n        # Signal playback finished - CRITICAL for session coordination\n        self.on_playback_finished(playback_position=played_duration, interrupted=interrupted)\n        \n        # Reset state for next segment\n        self._pushed_duration = 0.0\n        self._paused_at = None\n        self._paused_duration = 0.0\n        self._interrupted_ev.clear()\n        with self._lock:\n            self._output_empty_ev.set()\n    \n    def clear_buffer(self) -> None:\n        \"\"\"Clear the buffer and signal interruption.\"\"\"\n        with self._lock:\n            self._buffer.clear()\n            self._output_empty_ev.set()\n        \n        # Signal interruption if we were playing\n        if self._pushed_duration:\n            self._interrupted_ev.set()\n    \n    def pause(self) -> None:\n        \"\"\"Pause audio playback.\"\"\"\n        super().pause()\n        if self._paused_at is None:\n            self._paused_at = time.monotonic()\n            logger.debug(\"Playback paused\")\n    \n    def resume(self) -> None:\n        \"\"\"Resume audio playback.\"\"\"\n        super().resume()\n        if self._paused_at is not None:\n            self._paused_duration += time.monotonic() - self._paused_at\n            self._paused_at = None\n            logger.debug(\"Playback resumed\")\n    \n    def get_audio(self, num_bytes: int) -> bytes:\n        \"\"\"Get audio data for sounddevice output callback.\"\"\"\n        with self._lock:\n            # If paused, return silence\n            if self.paused:\n                return bytes(num_bytes)\n            \n            if len(self._buffer) >= num_bytes:\n                data = bytes(self._buffer[:num_bytes])\n                del self._buffer[:num_bytes]\n                return data\n            else:\n                # Return what we have + zero padding\n                data = bytes(self._buffer) + bytes(num_bytes - len(self._buffer))\n                self._buffer.clear()\n                # Mark empty in the event loop\n                try:\n                    self._loop.call_soon_threadsafe(self.mark_output_empty)\n                except RuntimeError:\n                    pass\n                return data\n    \n    def close(self):\n        self._closed = True\n        self.clear_buffer()\n\n\n# Need to import time for pause tracking\nimport time\n\nprint(\"Audio I/O classes defined (with proper playback tracking)\")\nprint(\"- on_playback_started() called when first audio frame received\")\nprint(\"- on_playback_finished() called when playback completes or is interrupted\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Create OpenAI Realtime Model\nrealtime_model = openai.realtime.RealtimeModel(\n    model=\"gpt-4o-realtime-preview\",\n    voice=\"alloy\",  # Options: alloy, echo, fable, onyx, nova, shimmer\n)\n\n# Create Agent with tools\nagent = Agent(\n    instructions=\"\"\"You are a helpful voice assistant. ALWAYS respond in English.\n\nYou have access to:\n1. **Calculator**: Evaluate math expressions (sqrt, sin, cos, log, pi, etc.)\n2. **Current Time**: Get the current date and time in any timezone\n3. **Slow Web Search**: Demo tool that takes 3 seconds - shows how the framework handles slow operations\n\nGuidelines:\n- ALWAYS speak in English, regardless of what language the user speaks\n- Be conversational and friendly\n- Keep responses concise (this is voice)\n- Use calculator for any math\n- When using slow_web_search, tell the user you're searching while you wait for results\n\"\"\",\n    llm=realtime_model,\n    tools=[\n        calculate,        # Custom math tool (fast)\n        get_current_time, # Custom time tool (fast)\n        slow_web_search,  # Slow tool demo (blocks with interruption support)\n    ],\n)\n\nprint(f\"Agent created with {len(agent.tools)} tools:\")\nprint(\"  - calculate (fast)\")  \nprint(\"  - get_current_time (fast)\")\nprint(\"  - slow_web_search (slow - 3s, with interruption support)\")\nprint()\nprint(f\"Model: gpt-4o-realtime-preview\")\nprint(f\"Voice: alloy\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Main Voice Assistant Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "async def run_voice_assistant(\n    input_device: int | str | None = None,\n    output_device: int | str | None = None,\n    duration: float | None = None,\n):\n    \"\"\"\n    Run the voice assistant with local audio I/O.\n    \n    Uses LiveKit's AudioProcessingModule for echo cancellation.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    \n    audio_input = NotebookAudioInput(loop)\n    audio_output = NotebookAudioOutput(loop)\n    \n    logger.info(f\"Audio output can_pause: {audio_output.can_pause}\")\n    \n    apm = rtc.AudioProcessingModule(\n        echo_cancellation=True,\n        noise_suppression=True,\n        high_pass_filter=True,\n        auto_gain_control=True,\n    )\n    print(\"Echo cancellation enabled via AudioProcessingModule\")\n    \n    session_active = True\n    input_delay = 0.0\n    output_delay = 0.0\n    \n    def input_callback(indata, frames, time_info, status):\n        nonlocal input_delay\n        if not session_active:\n            return\n        \n        input_delay = time_info.currentTime - time_info.inputBufferAdcTime\n        total_delay = output_delay + input_delay\n        try:\n            apm.set_stream_delay_ms(int(total_delay * 1000))\n        except RuntimeError:\n            pass\n        \n        num_frames = frames // FRAME_SAMPLES\n        for i in range(num_frames):\n            start = i * FRAME_SAMPLES\n            end = start + FRAME_SAMPLES\n            chunk = indata[start:end, 0]\n            \n            frame = rtc.AudioFrame(\n                data=chunk.tobytes(),\n                samples_per_channel=FRAME_SAMPLES,\n                sample_rate=SAMPLE_RATE,\n                num_channels=CHANNELS,\n            )\n            apm.process_stream(frame)\n            loop.call_soon_threadsafe(audio_input.push_frame, frame)\n    \n    def output_callback(outdata, frames, time_info, status):\n        nonlocal output_delay\n        if not session_active:\n            outdata[:] = 0\n            return\n            \n        output_delay = time_info.outputBufferDacTime - time_info.currentTime\n        num_bytes = frames * CHANNELS * 2\n        \n        with audio_output.audio_lock:\n            is_paused = audio_output.paused\n        \n        if is_paused:\n            outdata[:] = 0\n            silence = np.zeros(FRAME_SAMPLES, dtype=np.int16)\n            num_frames = frames // FRAME_SAMPLES\n            for i in range(num_frames):\n                render_frame = rtc.AudioFrame(\n                    data=silence.tobytes(),\n                    samples_per_channel=FRAME_SAMPLES,\n                    sample_rate=SAMPLE_RATE,\n                    num_channels=CHANNELS,\n                )\n                apm.process_reverse_stream(render_frame)\n            return\n        \n        data = audio_output.get_audio(num_bytes)\n        audio_samples = np.frombuffer(data, dtype=np.int16)\n        outdata[:, 0] = audio_samples\n        \n        num_frames = frames // FRAME_SAMPLES\n        for i in range(num_frames):\n            start = i * FRAME_SAMPLES\n            end = start + FRAME_SAMPLES\n            chunk = outdata[start:end, 0]\n            render_frame = rtc.AudioFrame(\n                data=chunk.tobytes(),\n                samples_per_channel=FRAME_SAMPLES,\n                sample_rate=SAMPLE_RATE,\n                num_channels=CHANNELS,\n            )\n            apm.process_reverse_stream(render_frame)\n    \n    if input_device is None:\n        input_device = sd.default.device[0]\n    if output_device is None:\n        output_device = sd.default.device[1]\n    \n    print(\"=\"*60)\n    print(\"OpenAI Voice Assistant Ready!\")\n    print(\"=\"*60)\n    print(f\"Input:  {sd.query_devices(input_device)['name']}\")\n    print(f\"Output: {sd.query_devices(output_device)['name']}\")\n    print()\n    print(\"Try saying:\")\n    print(\"  - 'What time is it?'\")\n    print(\"  - 'What is 25 times 17?'\")\n    print(\"  - 'Use slow search for AI news'\")\n    print(\"=\"*60)\n    print()\n    \n    input_stream = sd.InputStream(\n        callback=input_callback,\n        device=input_device,\n        channels=CHANNELS,\n        samplerate=SAMPLE_RATE,\n        blocksize=BLOCK_SIZE,\n        dtype='int16',\n    )\n    \n    output_stream = sd.OutputStream(\n        callback=output_callback,\n        device=output_device,\n        channels=CHANNELS,\n        samplerate=SAMPLE_RATE,\n        blocksize=BLOCK_SIZE,\n        dtype='int16',\n    )\n    \n    try:\n        input_stream.start()\n        output_stream.start()\n        print(\"Audio streams started\")\n        \n        session = AgentSession(\n            allow_interruptions=True,\n            min_interruption_duration=0.5,\n            min_interruption_words=0,\n            resume_false_interruption=True,\n            false_interruption_timeout=1.0,\n            min_endpointing_delay=0.5,\n            max_endpointing_delay=3.0,\n        )\n        \n        session.input.audio = audio_input\n        session.output.audio = audio_output\n        \n        @session.on(\"user_input_transcribed\")\n        def on_user_input(ev):\n            if ev.is_final:\n                print(f\"You: {ev.transcript}\")\n        \n        @session.on(\"agent_speech_transcribed\") \n        def on_agent_speech(ev):\n            if ev.is_final:\n                print(f\"Assistant: {ev.transcript}\")\n                print(\"---\")\n        \n        @session.on(\"function_tools_executed\")\n        def on_tools_executed(ev):\n            for call, output in ev.zipped():\n                print(f\"[Tool] {call.name}\")\n        \n        @session.on(\"error\")\n        def on_error(ev):\n            print(f\"[Error] {ev.error}\")\n\n        await session.start(agent=agent)\n        print(\"Session started\")\n        print()\n        \n        await session.generate_reply(\n            instructions=\"Greet the user briefly in English. Mention you can do math and tell time.\"\n        )\n        \n        if duration:\n            await asyncio.sleep(duration)\n        else:\n            while session_active:\n                await asyncio.sleep(1)\n    \n    except asyncio.CancelledError:\n        print(\"\\nSession cancelled.\")\n    except KeyboardInterrupt:\n        print(\"\\nSession interrupted.\")\n    except Exception as e:\n        print(f\"\\nError: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        session_active = False\n        input_stream.stop()\n        output_stream.stop()\n        input_stream.close()\n        output_stream.close()\n        audio_input.close()\n        audio_output.close()\n        print(\"Session ended.\")\n\n\nprint(\"Voice assistant function defined. Run the next cell to start!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Run the Voice Assistant\n",
    "\n",
    "Run the cell below to start. Speak into your microphone!\n",
    "\n",
    "**To stop:** Press the stop button in Jupyter or interrupt the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the voice assistant\n",
    "# Set duration=60 for 60 seconds, or None to run indefinitely\n",
    "await run_voice_assistant(duration=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "## Notes\n\n### Available OpenAI Voices\nalloy, echo, fable, onyx, nova, shimmer, ash, ballad, coral, sage, verse\n\n### Tools\n- **calculate**: Math expressions with sqrt, sin, cos, log, pi, e\n- **get_current_time**: Current time in any timezone\n\n### Troubleshooting\n- **No audio**: Check microphone permissions and device selection\n- **API errors**: Verify `OPENAI_API_KEY` is set\n- **PortAudio errors**: `brew install portaudio` on macOS\n\n### Echo Cancellation\nThe key to reliable local audio I/O is **echo cancellation**. Without it, the microphone picks up speaker output, causing:\n- False speech detection (server thinks user is speaking when agent is)\n- Interrupted responses (\"speech not done in time after interruption\")\n\nThis notebook uses `rtc.AudioProcessingModule` from LiveKit:\n- `process_reverse_stream()` - feed output audio as AEC reference\n- `process_stream()` - remove echo from microphone input\n\nThis is the same approach used by LiveKit's official `console` mode."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}