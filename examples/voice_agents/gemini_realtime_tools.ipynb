{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Realtime Voice Agent with Tools (LiveKit Agents)\n",
    "\n",
    "This notebook demonstrates a real-time voice assistant using **LiveKit Agents** framework with:\n",
    "- **Gemini Realtime API**: Native audio model with server-side turn detection\n",
    "- **Echo Cancellation**: Uses LiveKit's AudioProcessingModule to prevent feedback\n",
    "- **GoogleSearch**: Built-in Gemini tool for live web search (grounding)\n",
    "- **Custom function tools**: Calculator, current time\n",
    "- **Local audio I/O**: Microphone input and speaker output via sounddevice\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```bash\n",
    "pip install sounddevice livekit livekit-agents livekit-plugins-google python-dotenv\n",
    "```\n",
    "\n",
    "**macOS:** You may need PortAudio:\n",
    "```bash\n",
    "brew install portaudio\n",
    "```\n",
    "\n",
    "## Usage\n",
    "1. Set `GOOGLE_GEMINI_API_KEY` environment variable\n",
    "2. Run all cells in order\n",
    "3. Speak into your microphone\n",
    "4. Try: \"Search for the latest AI news\" or \"What's 25 times 17?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install sounddevice livekit livekit-agents livekit-plugins-google python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 24000 Hz\n",
      "Block size: 2400 samples (100ms)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import math\n",
    "import threading\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from livekit import rtc\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging (INFO level - set to DEBUG for verbose output)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"voice_agent\")\n",
    "\n",
    "# Audio configuration (matching LiveKit agents console mode)\n",
    "SAMPLE_RATE = 24000\n",
    "CHANNELS = 1\n",
    "FRAME_SAMPLES = 240  # 10ms frames\n",
    "BLOCK_SIZE = 2400    # 100ms blocks\n",
    "\n",
    "print(f\"Sample rate: {SAMPLE_RATE} Hz\")\n",
    "print(f\"Block size: {BLOCK_SIZE} samples ({BLOCK_SIZE/SAMPLE_RATE*1000:.0f}ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found!\n"
     ]
    }
   ],
   "source": [
    "# Verify API key\n",
    "api_key = os.environ.get(\"GOOGLE_GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"GOOGLE_GEMINI_API_KEY not set.\\n\"\n",
    "        \"Set it with: export GOOGLE_GEMINI_API_KEY='your-key'\"\n",
    "    )\n",
    "print(\"API key found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio devices:\n",
      "> 0 My boss, Core Audio (1 in, 0 out)\n",
      "< 1 My boss, Core Audio (0 in, 2 out)\n",
      "  2 YZâ€™s iPhone Microphone, Core Audio (1 in, 0 out)\n",
      "  3 MacBook Air Microphone, Core Audio (1 in, 0 out)\n",
      "  4 MacBook Air Speakers, Core Audio (0 in, 2 out)\n",
      "\n",
      "Default input: 0\n",
      "Default output: 1\n"
     ]
    }
   ],
   "source": [
    "# List available audio devices\n",
    "print(\"Available audio devices:\")\n",
    "print(sd.query_devices())\n",
    "print(f\"\\nDefault input: {sd.default.device[0]}\")\n",
    "print(f\"Default output: {sd.default.device[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LiveKit Agents Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiveKit Agents components imported!\n"
     ]
    }
   ],
   "source": [
    "from livekit.agents import Agent, AgentSession, utils\n",
    "from livekit.agents.voice import io\n",
    "from livekit.agents.voice.io import AudioOutputCapabilities\n",
    "from livekit.agents.voice.events import RunContext  # For tool context\n",
    "from livekit.agents.llm import function_tool\n",
    "from livekit.plugins import google\n",
    "from livekit.plugins.google.tools import GoogleSearch\n",
    "from livekit import rtc\n",
    "\n",
    "print(\"LiveKit Agents components imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools defined:\n",
      "  - calculate: Fast math evaluation\n",
      "  - get_current_time: Fast time lookup\n",
      "  - slow_web_search: Slow search (3s) with interruption support\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Custom Tools\n",
    "# =============================================================================\n",
    "# \n",
    "# NOTE: The LiveKit Agents framework uses a state machine that pauses speech\n",
    "# scheduling after the agent finishes speaking. This means background tasks\n",
    "# CANNOT inject speech after the main turn completes.\n",
    "#\n",
    "# The supported patterns are:\n",
    "# 1. Fast tools: Return immediately (like calculate, get_current_time)\n",
    "# 2. Slow tools: Block until completion (framework handles the flow)\n",
    "#\n",
    "# For slow operations, the agent will naturally speak acknowledgment\n",
    "# (\"Let me search for that...\") while the tool runs.\n",
    "# =============================================================================\n",
    "\n",
    "@function_tool\n",
    "async def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression.\n",
    "    \n",
    "    Args:\n",
    "        expression: Math expression (e.g., '2 + 2', 'sqrt(16)', 'sin(pi/2)')\n",
    "    \"\"\"\n",
    "    allowed = {\n",
    "        'sqrt': math.sqrt, 'sin': math.sin, 'cos': math.cos,\n",
    "        'tan': math.tan, 'log': math.log, 'log10': math.log10,\n",
    "        'exp': math.exp, 'pi': math.pi, 'e': math.e,\n",
    "        'abs': abs, 'round': round, 'pow': pow,\n",
    "    }\n",
    "    try:\n",
    "        result = eval(expression, {'__builtins__': {}}, allowed)\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "@function_tool\n",
    "async def get_current_time(timezone: str = \"local\") -> str:\n",
    "    \"\"\"Get the current date and time.\n",
    "    \n",
    "    Args:\n",
    "        timezone: Timezone name (e.g., 'UTC', 'US/Pacific'). Defaults to local.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if timezone and timezone != \"local\":\n",
    "            import pytz\n",
    "            tz = pytz.timezone(timezone)\n",
    "            now = datetime.now(tz)\n",
    "            return f\"The time in {timezone} is {now.strftime('%Y-%m-%d %H:%M:%S %Z')}\"\n",
    "        else:\n",
    "            now = datetime.now()\n",
    "            return f\"The local time is {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    except Exception:\n",
    "        now = datetime.utcnow()\n",
    "        return f\"The UTC time is {now.strftime('%Y-%m-%d %H:%M:%S')} UTC\"\n",
    "\n",
    "\n",
    "@function_tool\n",
    "async def slow_web_search(ctx: RunContext, query: str) -> str | None:\n",
    "    \"\"\"Search the web for information (demonstrates slow tool handling).\n",
    "    \n",
    "    This tool simulates a slow web search that takes 3 seconds.\n",
    "    The framework handles the flow: agent can speak while this runs,\n",
    "    user can interrupt, and result is spoken when ready.\n",
    "    \n",
    "    Args:\n",
    "        ctx: RunContext for speech handle access\n",
    "        query: The search query\n",
    "    \"\"\"\n",
    "    print(f\"[Tool] slow_web_search starting for: {query}\")\n",
    "    \n",
    "    # Create the slow task\n",
    "    async def _do_search():\n",
    "        await asyncio.sleep(3)  # Simulate API delay\n",
    "        return f\"Top results for '{query}': 1) AI advances in 2024, 2) New language models released, 3) Major tech announcements\"\n",
    "    \n",
    "    # Start the task\n",
    "    search_task = asyncio.ensure_future(_do_search())\n",
    "    \n",
    "    # Wait for either: task completion OR user interruption\n",
    "    # This lets the agent speak naturally while we wait\n",
    "    await ctx.speech_handle.wait_if_not_interrupted([search_task])\n",
    "    \n",
    "    if ctx.speech_handle.interrupted:\n",
    "        print(f\"[Tool] slow_web_search interrupted for: {query}\")\n",
    "        search_task.cancel()\n",
    "        return None  # Return None to skip tool reply\n",
    "    \n",
    "    result = search_task.result()\n",
    "    print(f\"[Tool] slow_web_search completed for: {query}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Tools defined:\")\n",
    "print(\"  - calculate: Fast math evaluation\")\n",
    "print(\"  - get_current_time: Fast time lookup\")\n",
    "print(\"  - slow_web_search: Slow search (3s) with interruption support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Transfer (Multi-Agent Handoff)\n",
    "\n",
    "This demonstrates agent transfer with Gemini Realtime. When the user asks for specialized help,\n",
    "the main agent can transfer to a specialist agent.\n",
    "\n",
    "**How it works:**\n",
    "- A tool returns an `Agent` (or `tuple[Agent, str]`) to trigger handoff\n",
    "- The framework pauses the current agent and activates the new one\n",
    "- The new agent inherits the conversation context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent transfer setup complete!\n",
      "  - math_expert: Specialized agent for math (voice: Charon)\n",
      "  - transfer_to_math_expert: Tool to handoff to math expert\n",
      "  - transfer_to_main_agent: Tool to return to main assistant\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Multi-Agent Setup with Transfer\n",
    "# =============================================================================\n",
    "# \n",
    "# Agent transfer works by returning an Agent from a tool function.\n",
    "# The framework will pause the current agent and activate the new one.\n",
    "# =============================================================================\n",
    "\n",
    "# Store agents in a dict for easy lookup\n",
    "agents: dict[str, Agent] = {}\n",
    "\n",
    "# Get API key for creating realtime models\n",
    "gemini_api_key = os.environ.get(\"GOOGLE_GEMINI_API_KEY\")\n",
    "\n",
    "# Create a separate realtime model for the math expert (can use different voice)\n",
    "math_expert_model = google.realtime.RealtimeModel(\n",
    "    api_key=gemini_api_key,\n",
    "    model=\"gemini-2.5-flash-native-audio-preview-12-2025\",\n",
    "    voice=\"Charon\",  # Different voice for math expert\n",
    "    temperature=0.3,  # Lower temperature for precise math\n",
    ")\n",
    "\n",
    "# Tool to transfer back to main agent\n",
    "@function_tool\n",
    "async def transfer_to_main_agent() -> Agent:\n",
    "    \"\"\"Transfer the conversation back to the main assistant.\n",
    "    \n",
    "    Use this when you've finished helping with the specialized task\n",
    "    and the user wants general assistance again.\n",
    "    \"\"\"\n",
    "    print(\"[Transfer] Returning to main agent\")\n",
    "    return agents[\"main\"]\n",
    "\n",
    "\n",
    "# Math Expert Agent - specialized for complex calculations\n",
    "math_expert = Agent(\n",
    "    instructions=\"\"\"You are a MATH EXPERT assistant. You speak with confidence about mathematics.\n",
    "\n",
    "Your specialty is:\n",
    "- Complex mathematical calculations\n",
    "- Explaining mathematical concepts\n",
    "- Step-by-step problem solving\n",
    "- Statistical analysis\n",
    "\n",
    "Guidelines:\n",
    "- ALWAYS respond in English\n",
    "- Be precise and accurate\n",
    "- Explain your reasoning step by step\n",
    "- Use the calculate tool for actual computation\n",
    "- When the user is done with math questions, offer to transfer them back to the main assistant\n",
    "- Use transfer_to_main_agent when the user wants general help\n",
    "\n",
    "You have a more serious, professorial tone compared to the main assistant.\n",
    "\"\"\",\n",
    "    llm=math_expert_model,\n",
    "    tools=[\n",
    "        calculate,\n",
    "        transfer_to_main_agent,\n",
    "    ],\n",
    ")\n",
    "agents[\"math_expert\"] = math_expert\n",
    "\n",
    "\n",
    "# Tool to transfer to math expert\n",
    "@function_tool\n",
    "async def transfer_to_math_expert() -> tuple[Agent, str]:\n",
    "    \"\"\"Transfer the conversation to a math specialist.\n",
    "    \n",
    "    Use this when the user has complex math questions or needs\n",
    "    detailed mathematical explanations.\n",
    "    \"\"\"\n",
    "    print(\"[Transfer] Transferring to math expert\")\n",
    "    # Return tuple of (Agent, handoff message)\n",
    "    return agents[\"math_expert\"], \"Transferring you to our math expert who can help with complex calculations.\"\n",
    "\n",
    "\n",
    "print(\"Agent transfer setup complete!\")\n",
    "print(\"  - math_expert: Specialized agent for math (voice: Charon)\")\n",
    "print(\"  - transfer_to_math_expert: Tool to handoff to math expert\")\n",
    "print(\"  - transfer_to_main_agent: Tool to return to main assistant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio I/O Classes\n",
    "\n",
    "These classes connect sounddevice to the LiveKit Agents framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio I/O classes defined (with proper playback tracking)\n",
      "- on_playback_started() called when first audio frame received\n",
      "- on_playback_finished() called when playback completes or is interrupted\n"
     ]
    }
   ],
   "source": [
    "class NotebookAudioInput(io.AudioInput):\n",
    "    \"\"\"Audio input from microphone via sounddevice.\"\"\"\n",
    "    \n",
    "    def __init__(self, loop: asyncio.AbstractEventLoop):\n",
    "        super().__init__(label=\"Notebook Microphone\")\n",
    "        self._loop = loop\n",
    "        self._audio_ch: utils.aio.Chan[rtc.AudioFrame] = utils.aio.Chan()\n",
    "        self._attached = True\n",
    "    \n",
    "    def push_frame(self, frame: rtc.AudioFrame) -> None:\n",
    "        \"\"\"Push audio frame from sounddevice callback.\"\"\"\n",
    "        if self._attached:\n",
    "            try:\n",
    "                self._audio_ch.send_nowait(frame)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    async def __anext__(self) -> rtc.AudioFrame:\n",
    "        return await self._audio_ch.__anext__()\n",
    "    \n",
    "    def close(self):\n",
    "        self._attached = False\n",
    "        self._audio_ch.close()\n",
    "\n",
    "\n",
    "class NotebookAudioOutput(io.AudioOutput):\n",
    "    \"\"\"Audio output to speaker via sounddevice.\n",
    "    \n",
    "    Supports pause/resume for false interruption handling.\n",
    "    Properly tracks playback state with on_playback_started/on_playback_finished.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loop: asyncio.AbstractEventLoop):\n",
    "        super().__init__(\n",
    "            label=\"Notebook Speaker\",\n",
    "            capabilities=io.AudioOutputCapabilities(pause=True),  # Enable pause support\n",
    "            next_in_chain=None,\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "        )\n",
    "        self._loop = loop\n",
    "        self._buffer = bytearray()\n",
    "        self._lock = threading.Lock()\n",
    "        self._closed = False\n",
    "        \n",
    "        # Playback tracking - CRITICAL for proper session coordination\n",
    "        self._pushed_duration: float = 0.0\n",
    "        self._capture_start: float = 0.0\n",
    "        self._flush_task: asyncio.Task | None = None\n",
    "        self._output_empty_ev = asyncio.Event()\n",
    "        self._output_empty_ev.set()\n",
    "        self._interrupted_ev = asyncio.Event()\n",
    "        \n",
    "        # Pause tracking for false interruption handling\n",
    "        self._paused_at: float | None = None\n",
    "        self._paused_duration: float = 0.0\n",
    "    \n",
    "    @property\n",
    "    def paused(self) -> bool:\n",
    "        \"\"\"Check if audio output is paused.\"\"\"\n",
    "        return self._paused_at is not None\n",
    "    \n",
    "    @property\n",
    "    def audio_lock(self) -> threading.Lock:\n",
    "        return self._lock\n",
    "    \n",
    "    @property\n",
    "    def audio_buffer(self) -> bytearray:\n",
    "        return self._buffer\n",
    "    \n",
    "    def mark_output_empty(self) -> None:\n",
    "        \"\"\"Signal that output buffer is empty.\"\"\"\n",
    "        self._output_empty_ev.set()\n",
    "    \n",
    "    async def capture_frame(self, frame: rtc.AudioFrame) -> None:\n",
    "        \"\"\"Capture audio frame from agent for playback.\"\"\"\n",
    "        await super().capture_frame(frame)\n",
    "        if self._closed:\n",
    "            return\n",
    "        \n",
    "        # Wait for any pending flush to complete\n",
    "        if self._flush_task and not self._flush_task.done():\n",
    "            logger.warning(\"capture_frame called while flush in progress\")\n",
    "            await self._flush_task\n",
    "        \n",
    "        # Signal playback started on first frame\n",
    "        if not self._pushed_duration:\n",
    "            self._capture_start = time.monotonic()\n",
    "            self.on_playback_started(created_at=time.time())\n",
    "            logger.debug(\"Playback started\")\n",
    "        \n",
    "        # Track total pushed duration and add to buffer\n",
    "        self._pushed_duration += frame.duration\n",
    "        with self._lock:\n",
    "            self._buffer.extend(frame.data)\n",
    "            self._output_empty_ev.clear()\n",
    "    \n",
    "    def flush(self) -> None:\n",
    "        \"\"\"Flush buffered audio, marking segment complete.\"\"\"\n",
    "        super().flush()\n",
    "        if self._pushed_duration:\n",
    "            if self._flush_task and not self._flush_task.done():\n",
    "                logger.warning(\"flush called while previous flush in progress\")\n",
    "                self._flush_task.cancel()\n",
    "            \n",
    "            # Wait for playout to complete\n",
    "            self._flush_task = asyncio.create_task(self._wait_for_playout())\n",
    "    \n",
    "    async def _wait_for_playout(self) -> None:\n",
    "        \"\"\"Wait for audio to finish playing, then signal playback_finished.\"\"\"\n",
    "        async def _wait_buffered_audio() -> None:\n",
    "            while len(self._buffer) > 0:\n",
    "                await self._output_empty_ev.wait()\n",
    "                await asyncio.sleep(0)\n",
    "        \n",
    "        wait_for_interruption = asyncio.create_task(self._interrupted_ev.wait())\n",
    "        wait_for_playout = asyncio.create_task(_wait_buffered_audio())\n",
    "        \n",
    "        try:\n",
    "            await asyncio.wait(\n",
    "                [wait_for_playout, wait_for_interruption],\n",
    "                return_when=asyncio.FIRST_COMPLETED,\n",
    "            )\n",
    "            interrupted = wait_for_interruption.done()\n",
    "        finally:\n",
    "            wait_for_playout.cancel()\n",
    "            wait_for_interruption.cancel()\n",
    "        \n",
    "        # Account for any paused time\n",
    "        if self._paused_at is not None:\n",
    "            self._paused_duration += time.monotonic() - self._paused_at\n",
    "            self._paused_at = None\n",
    "        \n",
    "        # Calculate actual played duration\n",
    "        if interrupted:\n",
    "            played_duration = time.monotonic() - self._capture_start - self._paused_duration\n",
    "            played_duration = min(max(0, played_duration), self._pushed_duration)\n",
    "            logger.debug(f\"Playback interrupted after {played_duration:.2f}s\")\n",
    "        else:\n",
    "            played_duration = self._pushed_duration\n",
    "            logger.debug(f\"Playback completed: {played_duration:.2f}s\")\n",
    "        \n",
    "        # Signal playback finished - CRITICAL for session coordination\n",
    "        self.on_playback_finished(playback_position=played_duration, interrupted=interrupted)\n",
    "        \n",
    "        # Reset state for next segment\n",
    "        self._pushed_duration = 0.0\n",
    "        self._paused_at = None\n",
    "        self._paused_duration = 0.0\n",
    "        self._interrupted_ev.clear()\n",
    "        with self._lock:\n",
    "            self._output_empty_ev.set()\n",
    "    \n",
    "    def clear_buffer(self) -> None:\n",
    "        \"\"\"Clear the buffer and signal interruption.\"\"\"\n",
    "        with self._lock:\n",
    "            self._buffer.clear()\n",
    "            self._output_empty_ev.set()\n",
    "        \n",
    "        # Signal interruption if we were playing\n",
    "        if self._pushed_duration:\n",
    "            self._interrupted_ev.set()\n",
    "    \n",
    "    def pause(self) -> None:\n",
    "        \"\"\"Pause audio playback.\"\"\"\n",
    "        super().pause()\n",
    "        if self._paused_at is None:\n",
    "            self._paused_at = time.monotonic()\n",
    "            logger.debug(\"Playback paused\")\n",
    "    \n",
    "    def resume(self) -> None:\n",
    "        \"\"\"Resume audio playback.\"\"\"\n",
    "        super().resume()\n",
    "        if self._paused_at is not None:\n",
    "            self._paused_duration += time.monotonic() - self._paused_at\n",
    "            self._paused_at = None\n",
    "            logger.debug(\"Playback resumed\")\n",
    "    \n",
    "    def get_audio(self, num_bytes: int) -> bytes:\n",
    "        \"\"\"Get audio data for sounddevice output callback.\"\"\"\n",
    "        with self._lock:\n",
    "            # If paused, return silence\n",
    "            if self.paused:\n",
    "                return bytes(num_bytes)\n",
    "            \n",
    "            if len(self._buffer) >= num_bytes:\n",
    "                data = bytes(self._buffer[:num_bytes])\n",
    "                del self._buffer[:num_bytes]\n",
    "                return data\n",
    "            else:\n",
    "                # Return what we have + zero padding\n",
    "                data = bytes(self._buffer) + bytes(num_bytes - len(self._buffer))\n",
    "                self._buffer.clear()\n",
    "                # Mark empty in the event loop\n",
    "                try:\n",
    "                    self._loop.call_soon_threadsafe(self.mark_output_empty)\n",
    "                except RuntimeError:\n",
    "                    pass\n",
    "                return data\n",
    "    \n",
    "    def close(self):\n",
    "        self._closed = True\n",
    "        self.clear_buffer()\n",
    "\n",
    "\n",
    "# Need to import time for pause tracking\n",
    "import time\n",
    "\n",
    "print(\"Audio I/O classes defined (with proper playback tracking)\")\n",
    "print(\"- on_playback_started() called when first audio frame received\")\n",
    "print(\"- on_playback_finished() called when playback completes or is interrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main agent created with 5 tools:\n",
      "  - GoogleSearch (built-in)\n",
      "  - calculate (fast)\n",
      "  - get_current_time (fast)\n",
      "  - slow_web_search (slow - 3s, with interruption support)\n",
      "  - transfer_to_math_expert (agent handoff)\n",
      "\n",
      "Model: gemini-2.5-flash-native-audio-preview-12-2025\n",
      "Voice: Puck\n",
      "Turn detection: Server-side (default)\n",
      "\n",
      "Agent Transfer Test:\n",
      "  Say 'I need help with complex math' to trigger transfer to math expert\n",
      "  The math expert uses voice 'Charon' - you should hear a different voice!\n"
     ]
    }
   ],
   "source": [
    "# Create Gemini Realtime Model for main agent\n",
    "# Using default server-side turn detection (works well with echo cancellation)\n",
    "realtime_model = google.realtime.RealtimeModel(\n",
    "    api_key=gemini_api_key,\n",
    "    model=\"gemini-2.5-flash-native-audio-preview-12-2025\",\n",
    "    voice=\"Puck\",\n",
    "    temperature=0.8,\n",
    "    # Default uses server-side turn detection\n",
    ")\n",
    "\n",
    "# Create Main Agent with tools including transfer capability\n",
    "agent = Agent(\n",
    "    instructions=\"\"\"You are a helpful voice assistant. ALWAYS respond in English.\n",
    "\n",
    "You have access to:\n",
    "1. **Google Search**: Built-in Gemini web search for current information\n",
    "2. **Calculator**: Evaluate math expressions (sqrt, sin, cos, log, pi, etc.)\n",
    "3. **Current Time**: Get the current date and time in any timezone\n",
    "4. **Slow Web Search**: Demo tool that takes 3 seconds - shows how the framework handles slow operations\n",
    "5. **Math Expert Transfer**: Transfer to a specialized math expert for complex calculations\n",
    "\n",
    "Guidelines:\n",
    "- ALWAYS speak in English, regardless of what language the user speaks\n",
    "- Be conversational and friendly\n",
    "- Keep responses concise (this is voice)\n",
    "- Use Google Search for factual questions or current events\n",
    "- Use calculator for simple math\n",
    "- For COMPLEX math questions or when the user wants detailed mathematical explanations,\n",
    "  use transfer_to_math_expert to hand them off to our math specialist\n",
    "- When using slow_web_search, tell the user you're searching while you wait for results\n",
    "\"\"\",\n",
    "    llm=realtime_model,\n",
    "    tools=[\n",
    "        GoogleSearch(),          # Built-in Gemini web search\n",
    "        calculate,               # Custom math tool (fast)\n",
    "        get_current_time,        # Custom time tool (fast)\n",
    "        slow_web_search,         # Slow tool demo (blocks with interruption support)\n",
    "        transfer_to_math_expert, # Agent transfer to math specialist\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Store main agent in the agents dict for transfer back\n",
    "agents[\"main\"] = agent\n",
    "\n",
    "print(f\"Main agent created with {len(agent.tools)} tools:\")\n",
    "print(\"  - GoogleSearch (built-in)\")\n",
    "print(\"  - calculate (fast)\")  \n",
    "print(\"  - get_current_time (fast)\")\n",
    "print(\"  - slow_web_search (slow - 3s, with interruption support)\")\n",
    "print(\"  - transfer_to_math_expert (agent handoff)\")\n",
    "print()\n",
    "print(f\"Model: {realtime_model.model}\")\n",
    "print(f\"Voice: Puck\")\n",
    "print(f\"Turn detection: Server-side (default)\")\n",
    "print()\n",
    "print(\"Agent Transfer Test:\")\n",
    "print(\"  Say 'I need help with complex math' to trigger transfer to math expert\")\n",
    "print(\"  The math expert uses voice 'Charon' - you should hear a different voice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Voice Assistant Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def run_voice_assistant(\n    input_device: int | str | None = None,\n    output_device: int | str | None = None,\n    duration: float | None = None,\n):\n    \"\"\"\n    Run the voice assistant with local audio I/O.\n    \n    Uses LiveKit's AudioProcessingModule for echo cancellation to prevent\n    the microphone from picking up speaker output.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    \n    audio_input = NotebookAudioInput(loop)\n    audio_output = NotebookAudioOutput(loop)\n    \n    logger.info(f\"Audio output can_pause: {audio_output.can_pause}\")\n    \n    apm = rtc.AudioProcessingModule(\n        echo_cancellation=True,\n        noise_suppression=True,\n        high_pass_filter=True,\n        auto_gain_control=True,\n    )\n    print(\"Echo cancellation enabled via AudioProcessingModule\")\n    \n    session_active = True\n    input_delay = 0.0\n    output_delay = 0.0\n    is_first_agent = True  # Track if this is the initial agent start\n    \n    def input_callback(indata, frames, time_info, status):\n        nonlocal input_delay\n        if not session_active:\n            return\n        \n        input_delay = time_info.currentTime - time_info.inputBufferAdcTime\n        total_delay = output_delay + input_delay\n        try:\n            apm.set_stream_delay_ms(int(total_delay * 1000))\n        except RuntimeError:\n            pass\n        \n        num_frames = frames // FRAME_SAMPLES\n        for i in range(num_frames):\n            start = i * FRAME_SAMPLES\n            end = start + FRAME_SAMPLES\n            chunk = indata[start:end, 0]\n            \n            frame = rtc.AudioFrame(\n                data=chunk.tobytes(),\n                samples_per_channel=FRAME_SAMPLES,\n                sample_rate=SAMPLE_RATE,\n                num_channels=CHANNELS,\n            )\n            apm.process_stream(frame)\n            loop.call_soon_threadsafe(audio_input.push_frame, frame)\n    \n    def output_callback(outdata, frames, time_info, status):\n        nonlocal output_delay\n        if not session_active:\n            outdata[:] = 0\n            return\n            \n        output_delay = time_info.outputBufferDacTime - time_info.currentTime\n        num_bytes = frames * CHANNELS * 2\n        \n        with audio_output.audio_lock:\n            is_paused = audio_output.paused\n        \n        if is_paused:\n            outdata[:] = 0\n            silence = np.zeros(FRAME_SAMPLES, dtype=np.int16)\n            num_frames = frames // FRAME_SAMPLES\n            for i in range(num_frames):\n                render_frame = rtc.AudioFrame(\n                    data=silence.tobytes(),\n                    samples_per_channel=FRAME_SAMPLES,\n                    sample_rate=SAMPLE_RATE,\n                    num_channels=CHANNELS,\n                )\n                apm.process_reverse_stream(render_frame)\n            return\n        \n        data = audio_output.get_audio(num_bytes)\n        audio_samples = np.frombuffer(data, dtype=np.int16)\n        outdata[:, 0] = audio_samples\n        \n        num_frames = frames // FRAME_SAMPLES\n        for i in range(num_frames):\n            start = i * FRAME_SAMPLES\n            end = start + FRAME_SAMPLES\n            chunk = outdata[start:end, 0]\n            render_frame = rtc.AudioFrame(\n                data=chunk.tobytes(),\n                samples_per_channel=FRAME_SAMPLES,\n                sample_rate=SAMPLE_RATE,\n                num_channels=CHANNELS,\n            )\n            apm.process_reverse_stream(render_frame)\n    \n    if input_device is None:\n        input_device = sd.default.device[0]\n    if output_device is None:\n        output_device = sd.default.device[1]\n    \n    print(\"=\"*60)\n    print(\"Gemini Voice Assistant Ready!\")\n    print(\"=\"*60)\n    print(f\"Input:  {sd.query_devices(input_device)['name']}\")\n    print(f\"Output: {sd.query_devices(output_device)['name']}\")\n    print()\n    print(\"Try saying:\")\n    print(\"  - 'What time is it?'\")\n    print(\"  - 'What is 25 times 17?'\")\n    print(\"  - 'I need help with complex math' (transfers to math expert!)\")\n    print(\"  - 'Use slow search for AI news'\")\n    print(\"=\"*60)\n    print()\n    \n    input_stream = sd.InputStream(\n        callback=input_callback,\n        device=input_device,\n        channels=CHANNELS,\n        samplerate=SAMPLE_RATE,\n        blocksize=BLOCK_SIZE,\n        dtype='int16',\n    )\n    \n    output_stream = sd.OutputStream(\n        callback=output_callback,\n        device=output_device,\n        channels=CHANNELS,\n        samplerate=SAMPLE_RATE,\n        blocksize=BLOCK_SIZE,\n        dtype='int16',\n    )\n    \n    try:\n        input_stream.start()\n        output_stream.start()\n        print(\"Audio streams started\")\n        \n        session = AgentSession(\n            allow_interruptions=True,\n            min_interruption_duration=0.5,\n            min_interruption_words=0,\n            resume_false_interruption=True,\n            false_interruption_timeout=1.0,\n            min_endpointing_delay=0.5,\n            max_endpointing_delay=3.0,\n        )\n        \n        session.input.audio = audio_input\n        session.output.audio = audio_output\n        \n        @session.on(\"user_input_transcribed\")\n        def on_user_input(ev):\n            if ev.is_final:\n                print(f\"You: {ev.transcript}\")\n        \n        @session.on(\"agent_speech_transcribed\") \n        def on_agent_speech(ev):\n            if ev.is_final:\n                print(f\"Assistant: {ev.transcript}\")\n                print(\"---\")\n        \n        @session.on(\"function_tools_executed\")\n        def on_tools_executed(ev):\n            for call, output in ev.zipped():\n                print(f\"[Tool] {call.name}\")\n        \n        @session.on(\"agent_started\")\n        def on_agent_started(ev):\n            nonlocal is_first_agent\n            current_agent = ev.agent\n            \n            # Skip greeting for the initial agent start (handled separately)\n            if is_first_agent:\n                is_first_agent = False\n                print(f\"[Agent] Initial agent started\")\n                return\n            \n            print(f\"[Agent] Transferred to new agent\")\n            \n            # Generate a greeting for the new agent after transfer\n            # Determine which agent this is and provide appropriate instructions\n            if current_agent is agents.get(\"math_expert\"):\n                greeting = \"You just took over from the main assistant. Briefly introduce yourself as the math expert and ask how you can help with their math question.\"\n            elif current_agent is agents.get(\"main\"):\n                greeting = \"You just returned from the math expert. Briefly welcome the user back and ask if there's anything else you can help with.\"\n            else:\n                greeting = \"Briefly introduce yourself to the user.\"\n            \n            # Schedule the greeting (async)\n            asyncio.create_task(session.generate_reply(instructions=greeting))\n        \n        @session.on(\"error\")\n        def on_error(ev):\n            print(f\"[Error] {ev.error}\")\n\n        await session.start(agent=agent)\n        print(\"Session started\")\n        print()\n        \n        await session.generate_reply(\n            instructions=\"Greet the user briefly in English. Mention you can search the web, do math, tell time, and transfer to a math expert for complex calculations.\"\n        )\n        \n        if duration:\n            await asyncio.sleep(duration)\n        else:\n            while session_active:\n                await asyncio.sleep(1)\n    \n    except asyncio.CancelledError:\n        print(\"\\nSession cancelled.\")\n    except KeyboardInterrupt:\n        print(\"\\nSession interrupted.\")\n    except Exception as e:\n        print(f\"\\nError: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        session_active = False\n        input_stream.stop()\n        output_stream.stop()\n        input_stream.close()\n        output_stream.close()\n        audio_input.close()\n        audio_output.close()\n        print(\"Session ended.\")\n\n\nprint(\"Voice assistant function defined. Run the next cell to start!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Voice Assistant\n",
    "\n",
    "Run the cell below to start. Speak into your microphone!\n",
    "\n",
    "**To stop:** Press the stop button in Jupyter or interrupt the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:voice_agent:Audio output can_pause: True\n",
      "INFO:livekit:livekit_ffi::server:139:livekit_ffi::server - initializing ffi server v0.12.42\n",
      "INFO:livekit:livekit_ffi::cabi:50:livekit_ffi::cabi - initializing ffi server v0.12.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Echo cancellation enabled via AudioProcessingModule\n",
      "============================================================\n",
      "Gemini Voice Assistant Ready!\n",
      "============================================================\n",
      "Input:  My boss\n",
      "Output: My boss\n",
      "\n",
      "Try saying:\n",
      "  - 'What time is it?'\n",
      "  - 'What is 25 times 17?'\n",
      "  - 'I need help with complex math' (transfers to math expert!)\n",
      "  - 'Use slow search for AI news'\n",
      "============================================================\n",
      "\n",
      "Audio streams started\n",
      "Session started\n",
      "\n",
      "You: I I I need to help with the complex mass.\n",
      "[Transfer] Transferring to math expert\n",
      "[Tool] transfer_to_math_expert\n",
      "You: hello.\n",
      "You: was 25 times 11.\n",
      "[Tool] calculate\n",
      "You: please transfer back to the assistant.\n",
      "[Transfer] Returning to main agent\n",
      "[Tool] transfer_to_main_agent\n",
      "You: Hallo.\n",
      "You: Oh nice. Thank you. Bye.\n"
     ]
    }
   ],
   "source": [
    "# Run the voice assistant\n",
    "# Set duration=60 for 60 seconds, or None to run indefinitely\n",
    "await run_voice_assistant(duration=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Run with Specific Devices\n",
    "\n",
    "Use this if you need to specify audio devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with specific devices (use device IDs from the list above)\n",
    "# await run_voice_assistant(input_device=0, output_device=1, duration=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Available Gemini Voices\n",
    "Puck (default), Charon, Kore, Fenrir, Aoede, Achernar, Alnilam, Callirrhoe, Zephyr\n",
    "\n",
    "### Tools\n",
    "- **GoogleSearch**: Built-in Gemini grounding with live web search\n",
    "- **calculate**: Math expressions with sqrt, sin, cos, log, pi, e\n",
    "- **get_current_time**: Current time in any timezone\n",
    "\n",
    "### Troubleshooting\n",
    "- **No audio**: Check microphone permissions and device selection\n",
    "- **API errors**: Verify `GOOGLE_GEMINI_API_KEY` is set\n",
    "- **PortAudio errors**: `brew install portaudio` on macOS\n",
    "\n",
    "### Echo Cancellation\n",
    "The key to reliable local audio I/O is **echo cancellation**. Without it, the microphone picks up speaker output, causing:\n",
    "- False speech detection (server thinks user is speaking when agent is)\n",
    "- Interrupted responses (\"speech not done in time after interruption\")\n",
    "\n",
    "This notebook uses `rtc.AudioProcessingModule` from LiveKit:\n",
    "- `process_reverse_stream()` - feed output audio as AEC reference\n",
    "- `process_stream()` - remove echo from microphone input\n",
    "\n",
    "This is the same approach used by LiveKit's official `console` mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}