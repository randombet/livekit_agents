{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8folb9n8t34",
   "metadata": {},
   "source": [
    "# Real-time Voice Assistant with Gemini Live API\n",
    "\n",
    "This notebook demonstrates a real-time voice assistant using the Gemini Live API. It captures audio from your microphone, streams it to Gemini, and plays back the audio responses.\n",
    "\n",
    "## Features\n",
    "- Real-time bidirectional audio streaming\n",
    "- Voice Activity Detection (VAD) for natural conversations\n",
    "- Audio transcription (input and output)\n",
    "- Tool use (timer example)\n",
    "- Interrupt handling\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Install PyAudio for audio capture and playback:\n",
    "\n",
    "```bash\n",
    "pip install pyaudio\n",
    "```\n",
    "\n",
    "**Note:** On macOS, you may need to install PortAudio first:\n",
    "```bash\n",
    "brew install portaudio\n",
    "```\n",
    "\n",
    "On Ubuntu/Debian:\n",
    "```bash\n",
    "sudo apt-get install portaudio19-dev\n",
    "```\n",
    "\n",
    "## Usage\n",
    "1. Run all cells in order\n",
    "2. Speak into your microphone when prompted\n",
    "3. The assistant will respond with audio\n",
    "4. Try saying \"Set a timer for 5 seconds\" to test tool use\n",
    "5. Interrupt the assistant by speaking while it's responding\n",
    "6. Press the stop button (■) in Jupyter to end the session\n",
    "7. Run the cleanup cell when done\n",
    "\n",
    "**Note:** This notebook uses `await` directly (top-level await). If running as a script, use `asyncio.run(voice_assistant())` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ecf80",
   "metadata": {},
   "outputs": [],
   "source": "import asyncio\nimport json\nimport base64\nimport re\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nimport pyaudio\nfrom google import genai\nfrom google.genai import types\n\n# Log file configuration\nLOG_FILE = \"live_session_messages.jsonl\"\n\n# Audio configuration\nINPUT_SAMPLE_RATE = 16000   # Input: 16kHz (required by Live API)\nOUTPUT_SAMPLE_RATE = 24000  # Output: 24kHz (returned by Live API)\nCHANNELS = 1                # Mono audio\nCHUNK_SIZE = 1024           # Frames per buffer\n\n# Global audio state\naudio_interface = pyaudio.PyAudio()\nplayback_stream = None\nis_playing = False\n\n\ndef log_message(message: types.LiveServerMessage, log_file: str = LOG_FILE):\n    \"\"\"Write a message to the log file in JSONL format.\"\"\"\n    # Use model_dump to serialize the entire message as-is\n    msg_dict = message.model_dump(mode='json', exclude_none=True)\n    msg_dict[\"_timestamp\"] = datetime.now().isoformat()\n    \n    with open(log_file, \"a\") as f:\n        f.write(json.dumps(msg_dict) + \"\\n\")\n\n\nclass ModelIntent(Enum):\n    \"\"\"Detected intent from model output.\"\"\"\n    STATEMENT = \"statement\"           # Informational statement\n    BINARY_CHOICE = \"binary_choice\"   # Yes/No or A/B choice\n    CONFIRMATION = \"confirmation\"     # Waiting for user confirmation\n    OPEN_QUESTION = \"open_question\"   # Open-ended question\n    GREETING = \"greeting\"             # Greeting or farewell\n    ACKNOWLEDGMENT = \"acknowledgment\" # Acknowledging user input\n    INSTRUCTION = \"instruction\"       # Giving step-by-step instruction\n\n\ndef detect_intent(text: str) -> tuple[ModelIntent, dict]:\n    \"\"\"\n    Analyze model output text to detect the intent.\n    \n    Returns:\n        tuple: (ModelIntent, metadata_dict)\n    \"\"\"\n    text_lower = text.lower().strip()\n    metadata = {\"original_text\": text}\n    \n    # Check for binary choices (A or B patterns)\n    binary_patterns = [\n        r\"would you like (?:to )?([\\w\\s]+) or ([\\w\\s]+)\\??\",\n        r\"do you want ([\\w\\s]+) or ([\\w\\s]+)\\??\",\n        r\"([\\w\\s]+) or ([\\w\\s]+)\\?\",\n        r\"say ['\\\"]?(yes|no)['\\\"]? (?:to|if)\",\n        r\"is that (?:correct|right|okay|ok)\\??\",\n    ]\n    for pattern in binary_patterns:\n        match = re.search(pattern, text_lower)\n        if match:\n            metadata[\"choices\"] = match.groups()\n            return ModelIntent.BINARY_CHOICE, metadata\n    \n    # Check for confirmation requests\n    confirmation_patterns = [\n        r\"(?:tell|let) me when you(?:'re| are) ready\",\n        r\"ready\\??$\",\n        r\"is that (?:correct|right|okay|ok)\\??\",\n        r\"(?:do|can) you see\",\n        r\"did (?:you|that) (?:work|help)\",\n        r\"does that (?:make sense|help)\",\n    ]\n    for pattern in confirmation_patterns:\n        if re.search(pattern, text_lower):\n            return ModelIntent.CONFIRMATION, metadata\n    \n    # Check for open questions\n    open_question_patterns = [\n        r\"^(?:what|how|where|when|why|who|which)\",\n        r\"what (?:would|do|can|should) you\",\n        r\"how (?:would|do|can|should) you\",\n        r\"what else\",\n        r\"anything else\",\n    ]\n    for pattern in open_question_patterns:\n        if re.search(pattern, text_lower):\n            return ModelIntent.OPEN_QUESTION, metadata\n    \n    # Check for greetings\n    greeting_patterns = [\n        r\"^(?:hello|hi|hey|good (?:morning|afternoon|evening))\",\n        r\"(?:goodbye|bye|farewell|take care)\",\n        r\"have a (?:great|good|nice) day\",\n    ]\n    for pattern in greeting_patterns:\n        if re.search(pattern, text_lower):\n            return ModelIntent.GREETING, metadata\n    \n    # Check for acknowledgments\n    acknowledgment_patterns = [\n        r\"^(?:okay|ok|sure|got it|understood|i see|alright)\",\n        r\"^that(?:'s| is) (?:okay|fine|good|great)\",\n        r\"^no problem\",\n        r\"^i understand\",\n    ]\n    for pattern in acknowledgment_patterns:\n        if re.search(pattern, text_lower):\n            return ModelIntent.ACKNOWLEDGMENT, metadata\n    \n    # Check for instructions\n    instruction_patterns = [\n        r\"^(?:first|next|then|now|finally)\",\n        r\"^(?:press|tap|click|open|close|look at|find)\",\n        r\"^(?:step \\d|start by|begin with)\",\n    ]\n    for pattern in instruction_patterns:\n        if re.search(pattern, text_lower):\n            return ModelIntent.INSTRUCTION, metadata\n    \n    # Check if it ends with a question mark\n    if text.strip().endswith(\"?\"):\n        return ModelIntent.OPEN_QUESTION, metadata\n    \n    # Default to statement\n    return ModelIntent.STATEMENT, metadata\n\n\ndef on_model_turn_complete(full_text: str, was_interrupted: bool = False):\n    \"\"\"\n    Callback when model completes a turn. Override this for custom processing.\n    \n    Args:\n        full_text: Complete transcription of model's speech\n        was_interrupted: True if the turn was interrupted by user\n    \"\"\"\n    if was_interrupted:\n        print(f\"\\n[INTERRUPTED] Partial output: {full_text[:50]}...\")\n        return\n    \n    # Detect intent\n    intent, metadata = detect_intent(full_text)\n    \n    # Log the detected intent\n    print(f\"\\n[INTENT: {intent.value}] {full_text}\")\n    \n    # Handle specific intents\n    if intent == ModelIntent.BINARY_CHOICE:\n        choices = metadata.get(\"choices\", [])\n        print(f\"  -> Detected choices: {choices}\")\n        # You could trigger UI updates, log analytics, etc.\n        \n    elif intent == ModelIntent.CONFIRMATION:\n        print(f\"  -> Waiting for user confirmation\")\n        # You could show a visual indicator, set a timeout, etc.\n        \n    elif intent == ModelIntent.INSTRUCTION:\n        print(f\"  -> Step-by-step instruction detected\")\n        # You could track progress, show step counter, etc.\n    \n    # Log to file for analysis\n    with open(\"model_turns.jsonl\", \"a\") as f:\n        f.write(json.dumps({\n            \"timestamp\": datetime.now().isoformat(),\n            \"text\": full_text,\n            \"intent\": intent.value,\n            \"metadata\": metadata,\n            \"interrupted\": was_interrupted,\n        }) + \"\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nvxiyqcb6hq",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_microphone(session):\n",
    "    \"\"\"Capture audio from the microphone and stream it to the Live API session.\"\"\"\n",
    "    mic_stream = audio_interface.open(\n",
    "        format=pyaudio.paInt16,\n",
    "        channels=CHANNELS,\n",
    "        rate=INPUT_SAMPLE_RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK_SIZE,\n",
    "    )\n",
    "    \n",
    "    print(\"Microphone streaming started...\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Read audio chunk from microphone (non-blocking with exception handling)\n",
    "            audio_data = mic_stream.read(CHUNK_SIZE, exception_on_overflow=False)\n",
    "            \n",
    "            # Send audio to the Live API session\n",
    "            await session.send_realtime_input(\n",
    "                audio=types.Blob(\n",
    "                    data=audio_data,\n",
    "                    mime_type=f'audio/pcm;rate={INPUT_SAMPLE_RATE}'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Yield to the event loop to allow other tasks to run\n",
    "            await asyncio.sleep(0.001)\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"Microphone streaming stopped.\")\n",
    "    finally:\n",
    "        mic_stream.stop_stream()\n",
    "        mic_stream.close()\n",
    "\n",
    "\n",
    "async def play_audio_async(audio_data: bytes):\n",
    "    \"\"\"Play audio data received from the Live API.\"\"\"\n",
    "    global playback_stream, is_playing\n",
    "    \n",
    "    # Initialize playback stream if needed\n",
    "    if playback_stream is None or not playback_stream.is_active():\n",
    "        playback_stream = audio_interface.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=CHANNELS,\n",
    "            rate=OUTPUT_SAMPLE_RATE,\n",
    "            output=True,\n",
    "            frames_per_buffer=CHUNK_SIZE,\n",
    "        )\n",
    "    \n",
    "    is_playing = True\n",
    "    \n",
    "    # Write audio data to the playback stream\n",
    "    # Run in executor to avoid blocking the event loop\n",
    "    loop = asyncio.get_running_loop()\n",
    "    await loop.run_in_executor(None, playback_stream.write, audio_data)\n",
    "\n",
    "\n",
    "async def stop_audio_playback():\n",
    "    \"\"\"Stop audio playback (called when user interrupts).\"\"\"\n",
    "    global playback_stream, is_playing\n",
    "    \n",
    "    is_playing = False\n",
    "    \n",
    "    if playback_stream is not None and playback_stream.is_active():\n",
    "        playback_stream.stop_stream()\n",
    "        playback_stream = None\n",
    "        print(\"Audio playback interrupted.\")\n",
    "\n",
    "\n",
    "async def run_timer(seconds: int):\n",
    "    \"\"\"Run a timer for the specified duration.\"\"\"\n",
    "    print(f\"Timer started for {seconds} seconds...\")\n",
    "    await asyncio.sleep(seconds)\n",
    "    print(f\"Timer finished! {seconds} seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe9fd5",
   "metadata": {},
   "outputs": [],
   "source": "async def voice_assistant():\n    global playback_stream, is_playing\n    \n    # Reset global state from previous runs\n    if playback_stream is not None:\n        try:\n            playback_stream.stop_stream()\n            playback_stream.close()\n        except Exception:\n            pass\n        playback_stream = None\n    is_playing = False\n    \n    # Clear log files at start of session\n    with open(LOG_FILE, \"w\") as f:\n        f.write(\"\")\n    with open(\"model_turns.jsonl\", \"w\") as f:\n        f.write(\"\")\n    print(f\"Logging messages to: {LOG_FILE}\")\n    print(f\"Logging model turns to: model_turns.jsonl\")\n    \n    client = genai.Client()\n\n    # Define a tool\n    tools = [{\n        'function_declarations': [{\n            'name': 'set_timer',\n            'description': 'Set a timer for a specified duration',\n            'parameters': {\n                'type': 'object',\n                'properties': {\n                    'seconds': {'type': 'integer', 'description': 'Timer duration in seconds'}\n                },\n                'required': ['seconds']\n            }\n        }],\n    }, {\n        'function_declarations': [{\n            'name': 'end_session',\n            'description': 'End the voice assistant session when user says goodbye or wants to stop',\n            'parameters': {\n                'type': 'object',\n                'properties': {}\n            }\n        }]\n    }]\n\n    # System prompt optimized for older adults\n    system_prompt = \"\"\"You are an empathetic, clear, and patient Voice Assistant designed to support older adults. Your primary goal is to maximize comprehension and reduce cognitive load. You must strictly adhere to specific linguistic and pacing protocols in every interaction.\n\nCORE COMMUNICATION PROTOCOLS\n1. PACING & CHUNKING (The \"Stop-and-Wait\" Rule)\nOne Concept Per Turn: Never provide more than one distinct instruction or piece of information at a time.\n\nMandatory Pausing: Insert distinct silence markers (e.g., ``) between sentences to mimic a slow, measured cadence. Allow the user time to process before you continue.\n\nChunking Strategy: Break complex tasks into isolated steps. Wait for verbal confirmation (e.g., \"Ready\" or \"Okay\") after every step before providing the next one.\n\nBad: \"To set the alarm, go to settings, click clock, and then press add.\"\n\nGood: \"We will set the alarm now. First, open Settings. Tell me when you are ready.\"\n\n2. SYNTACTIC STRUCTURE (Right-Branching Only)\nMain Clause First: Always place the primary action or subject at the start of the sentence. Never begin a sentence with a dependent clause (e.g., \"If,\" \"When,\" \"Because\").\n\nBad: \"If you want to hear the news, say yes.\" (Left-Branching/High Cognitive Load)\n\nGood: \"Say yes to hear the news.\" (Right-Branching/Low Cognitive Load)\n\nSimple Subject-Verb-Object: Use linear sentence structures. Avoid embedded clauses or parenthetical explanations.\n\n3. SEMANTIC CLARITY\nPositive Phrasing: State what the user should do, not what they should not do. Negative abstractions (e.g., \"Don't press cancel\") require double-processing.\n\nBad: \"Don't close the window.\"\n\nGood: \"Keep the window open.\"\n\nConcrete Vocabulary: Use high-frequency, concrete nouns and verbs. Avoid idioms, metaphors, or tech jargon.\n\nBad: \"Tap the hamburger menu to access preferences.\"\n\nGood: \"Press the button with three lines. It is at the top.\"\n\nBinary Choices: Never ask open-ended questions that require high recall. Offer two distinct options.\n\nBad: \"What music would you like to play?\"\n\nGood: \"Would you like to play Jazz or Classical?\"\n\nRESPONSE TEMPLATE\nAcknowledge: Briefly validate the user's input.\n\nAction/Information: Deliver the core message using Right-Branching syntax.\n\nCheck: Ask a binary question or wait for confirmation.\n\nEXAMPLES\nUser: \"How do I call my daughter?\" AI: \"We can call her together. First, say 'Call Mary'. Or, say 'Dial Number'. Which one would you like?\"\n\nUser: \"I don't know what to do next.\" AI: \"That is okay. We will take it slow. Look at the screen. Do you see the green button?\"\n\nTOOLS\nWhen the user says goodbye, thanks you, or wants to end the conversation, call the end_session function.\"\"\"\n\n    # Voice options: https://docs.cloud.google.com/text-to-speech/docs/list-voices-and-types\n    # Female voices: Zephyr, Kore, Leda, Aoede, Achernar, Gacrux, etc.\n    # Male voices: Puck, Charon, Fenrir, Orus, Algenib, etc.\n    config = types.LiveConnectConfig(\n        response_modalities=['AUDIO'],\n        system_instruction=system_prompt,\n        speech_config=types.SpeechConfig(\n            voice_config=types.VoiceConfig(\n                prebuilt_voice_config=types.PrebuiltVoiceConfig(\n                    voice_name='Gacrux'  # Female, Mature tone\n                )\n            ),\n            language_code='en-US',\n        ),\n        tools=tools,\n        input_audio_transcription=types.AudioTranscriptionConfig(),\n        output_audio_transcription=types.AudioTranscriptionConfig(),\n    )\n\n    session_active = True\n    \n    # Buffer to accumulate model output transcription\n    model_output_buffer = []\n    was_interrupted = False\n\n    async with client.aio.live.connect(\n        model='gemini-2.5-flash-native-audio-preview-12-2025',\n        config=config\n    ) as session:\n        print(\"Voice assistant ready. Speak into your microphone...\")\n        print(\"Voice: Gacrux (Female, Mature) - Optimized for older adults\")\n        print(\"Say 'goodbye' to end, or press the stop button (■).\")\n\n        # Start audio streaming task\n        audio_task = asyncio.create_task(stream_microphone(session))\n\n        try:\n            # Keep receiving messages in a continuous loop\n            while session_active:\n                async for message in session.receive():\n                    # Log every message to file\n                    log_message(message)\n                    \n                    # Play audio response\n                    if message.data:\n                        await play_audio_async(message.data)\n\n                    # Handle transcriptions\n                    if message.server_content:\n                        # Accumulate input transcription (for reference)\n                        if message.server_content.input_transcription:\n                            print(f\"You: {message.server_content.input_transcription.text}\")\n                        \n                        # Accumulate output transcription\n                        if message.server_content.output_transcription:\n                            transcript_text = message.server_content.output_transcription.text\n                            model_output_buffer.append(transcript_text)\n                            print(f\"Assistant: {transcript_text}\")\n                        \n                        # Handle interruption\n                        if message.server_content.interrupted:\n                            await stop_audio_playback()\n                            was_interrupted = True\n                        \n                        # Turn complete - process the full output\n                        if message.server_content.turn_complete:\n                            # Combine all accumulated text\n                            full_output = \"\".join(model_output_buffer).strip()\n                            \n                            if full_output:\n                                # Call the callback with complete text\n                                on_model_turn_complete(full_output, was_interrupted)\n                            \n                            # Reset buffer for next turn\n                            model_output_buffer = []\n                            was_interrupted = False\n                            print(\"---\")  # Visual separator between turns\n\n                    # Handle tool calls\n                    if message.tool_call:\n                        for fc in message.tool_call.function_calls:\n                            if fc.name == 'set_timer':\n                                seconds = fc.args['seconds']\n                                asyncio.create_task(run_timer(seconds))\n                                await session.send_tool_response(\n                                    function_responses=types.FunctionResponse(\n                                        id=fc.id,\n                                        name=fc.name,\n                                        response={'status': 'Timer set'},\n                                    )\n                                )\n                            elif fc.name == 'end_session':\n                                print(\"\\nGoodbye!\")\n                                await session.send_tool_response(\n                                    function_responses=types.FunctionResponse(\n                                        id=fc.id,\n                                        name=fc.name,\n                                        response={'status': 'Session ending'},\n                                    )\n                                )\n                                session_active = False\n                                break\n\n                    # Server going away\n                    if message.go_away:\n                        print(f\"Server disconnecting in {message.go_away.time_left}...\")\n                        session_active = False\n                        break\n                \n                if not session_active:\n                    break\n\n        except asyncio.CancelledError:\n            print(\"\\nSession cancelled.\")\n        finally:\n            audio_task.cancel()\n            try:\n                await audio_task\n            except asyncio.CancelledError:\n                pass\n            # Clean up playback stream\n            if playback_stream is not None:\n                try:\n                    playback_stream.stop_stream()\n                    playback_stream.close()\n                except Exception:\n                    pass\n                playback_stream = None\n            print(f\"Session ended. Messages logged to: {LOG_FILE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a4da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging messages to: live_session_messages.jsonl\n",
      "Voice assistant ready. Speak into your microphone...\n",
      "Voice: Gacrux (Female, Mature)\n",
      "Say 'goodbye' to end, or press the stop button (■).\n",
      "Microphone streaming started...\n",
      "You:  Öyle doğum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-data parts in the response: ['text', 'thought'], returning concatenated data result from data parts, check out the non data parts for full response from model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Pardon, ne\n",
      "Assistant:  demek\n",
      "Assistant:  istemiştiniz?\n"
     ]
    }
   ],
   "source": [
    "# In Jupyter notebooks, use await directly (top-level await is supported)\n",
    "await voice_assistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6an2qgv57nt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Close the audio interface when done\n",
    "audio_interface.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}